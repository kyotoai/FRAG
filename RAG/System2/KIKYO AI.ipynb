{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109a640f-af2b-4873-92c6-a380385a11fb",
   "metadata": {},
   "source": [
    "# Sample RAG system with KIKYO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11092c2a-c20d-4a50-af87-4c5cc6af5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this sample you have to check the following things\n",
    "#  what's the point of using SEAT, and how can it augument normal rag?\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b26197-acb9-46bd-aeac-e432183adda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m161.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m232.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m194.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.1 pyarrow-15.0.2 pyarrow-hotfix-0.6 pytz-2024.1 tqdm-4.66.2 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m167.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m197.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m224.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m263.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.29.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m177.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m200.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m181.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.39.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.24.1)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m236.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m162.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence_transformers\n",
      "Successfully installed joblib-1.4.0 scikit-learn-1.4.2 scipy-1.13.0 sentence_transformers-2.6.1 threadpoolctl-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfccefc-40db-4b69-b392-184d73a5d44e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Japanese LLM Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b1f60-149e-48f8-9f35-e99cce8b1ea4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Elizezen/Hameln-japanese-mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0cf8e2-4cc9-449b-bb61-32e0225178fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418b7840bd79404a888617e1c05387f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb8dd167c2249d78f56d5413a0963dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bba42a868dc438483c7feb1ffc418ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166b30a3bf954bdb936f83c77c9bbfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddc4d32925d4889bf2c5a3ef8029a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53deabccbae42eab6e6a738e419f441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55490c08cc1e4aabb56eb332cd1f4a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cc6d09163a46599d71c671aae3a7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab90499a8e24453ab94d98b2b608c4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e846b6e4ef4ae28f12b291a42f4a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d921b2d754499d82375bd19d73c16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c08e27fe03a4ff989d351b89d22653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "おばあさんは川へ洗濯へ行きました。しばらくして、二人はカスャカスャと奇怪な物音に気が付きました。\n",
      "「おじいさんや～、なんか音がすると思って来てみれば」\n",
      "「おばあさんや、足の腱を切られて動けんようじゃ」\n",
      "二人は呟きました。\n",
      "二人の近くでは、その狼が彼らを食べようとしているのです。今は自分たちの存在に気が付いていないが、いつ気が付かれるかわかりません。\n",
      "二人は怖くなって、その場を去りました。\n",
      "しばらくして、おばあさんが家の物置にしまってあった盥を持ってきました。そこに、斧を隠しておくつもりです。\n",
      "「その斧、使わずにすむと良いけどのぅ」\n",
      "「そうだねぇ」\n",
      "二人はそう呟いて、家の中で待機していると……。\n",
      "「カンダタ、カンダタ」\n",
      "「ははは、今日はぼぜ」\n",
      "「とっくん、とっくん」\n",
      "狼たちは、遊ぶように足の腱を切られたおじいさんを弄び、その姿を楽しんでいました。\n",
      "「ウガー！」\n",
      "盥に隠しておいた斧を取り出すと、それを投げつけます。\n",
      "「うわぁ！」\n",
      "狼に当たった斧は、狼の足を切り落とし、狼がその場で動けなくなります。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutput example:\\nむかしむかし、あるところに、おじいさんとおばあさんが住んでいました。 おじいさんは山へ柴刈りに、おばあさんは田んぼの稲の手伝いをするなど、二人で力を合わせて楽しく暮らしていました。\\nある日のこと、その地方一帯に大きな台風がやって来ました。強風に飛ばされた木や、家屋などが次々と倒れる中、幸いにもおじいさんとおばあさんの住んでいた村は無事でした。\\nしかし、近隣の小さな村では被害が出ていました。家屋は全壊、農作物は荒らされ、何より多くの命が失われていました。\\n「可哀想に……」\\nおばあさんは心を痛め、神様に祈りを捧げ続けました。\\n「天上の神様！どうか、私達人間を守って下さい！」\\nおばあさんの祈りが通じたのか、台風は急速に勢力を落とし、被害は最小限の内に治まりました。\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Elizezen/Hameln-japanese-mistral-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"Elizezen/Hameln-japanese-mistral-7B\",\n",
    "  torch_dtype=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    \"むかしむかし、あるところに、おじいさんとおばあさんが住んでいました。 おじいさんは山へ柴刈りに、\",\n",
    "    add_special_tokens=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    input_ids.to(device=model.device),\n",
    "    max_new_tokens=512,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "out = tokenizer.decode(tokens[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "print(out)\n",
    "\n",
    "\"\"\"\n",
    "output example:\n",
    "むかしむかし、あるところに、おじいさんとおばあさんが住んでいました。 おじいさんは山へ柴刈りに、おばあさんは田んぼの稲の手伝いをするなど、二人で力を合わせて楽しく暮らしていました。\n",
    "ある日のこと、その地方一帯に大きな台風がやって来ました。強風に飛ばされた木や、家屋などが次々と倒れる中、幸いにもおじいさんとおばあさんの住んでいた村は無事でした。\n",
    "しかし、近隣の小さな村では被害が出ていました。家屋は全壊、農作物は荒らされ、何より多くの命が失われていました。\n",
    "「可哀想に……」\n",
    "おばあさんは心を痛め、神様に祈りを捧げ続けました。\n",
    "「天上の神様！どうか、私達人間を守って下さい！」\n",
    "おばあさんの祈りが通じたのか、台風は急速に勢力を落とし、被害は最小限の内に治まりました。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6559dcfb-53fa-47a2-ab22-586ed8a0ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一般的に言ったら胸腔に空気を注入してあげるっていうのが一般的だとは思うんですけれども。\n",
      "いわゆるオペ室の酸素箱っていう機械に患者さんを寝かせて、患者さんの自然な呼吸の度に箱が開いて外気を取り込む仕組みになってるんですよね。\n",
      "それが中等度の気胸だと2回ぐらい空気を入れると患者さんにとっては安定してるっていうデータがあって。\n",
      "で、大抵の場合、そこまででおしまいです。ある程度の安定が確認されたら外科の先生にお返しして。\n",
      "でも先輩が私に言ったんです。\n",
      "「私はこの人は合計6回空気を入れてやっと安定したから、私も6回空気を入れてあげなさい。」って。\n",
      "だから最初は不思議に思っていたんですけどね。\n",
      "それで一応6回ほど試して見てから、お昼の休憩をとったんです。\n",
      "休憩中にもう一度患者さんのカルテを再確認してみたんです。\n",
      "…………先輩はこの患者さんのカルテを勝手に改ざんしてた。\n",
      "カルテには『外科医から胸腔ドレーンを外すタイミングは合計6回空気を入れた時です。』という指示が書かれていたんです。\n",
      "私には分かってました。\n",
      "先輩はこの��\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"中等度の気胸になった場合、どのように治療するのが一般的ですか？\",\n",
    "    add_special_tokens=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    input_ids.to(device=model.device),\n",
    "    max_new_tokens=512,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "out = tokenizer.decode(tokens[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0187cb64-28d3-4b4f-949b-14f0c2f8dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「はい。ええと……えっと…………\n",
      "　……ええとっ…………\n",
      "　……ええと…………\n",
      "　……あっ、いや、その、なんだ……\n",
      "　……胸を……刺す……\n",
      "　……そうだよ。ええと……\n",
      "　……浩一くんにはしないし、出来ないけど……\n",
      "　……ええとっ、胸筋を刺すんだよ……\n",
      "　……そう。それは、パライバスグリーン……」\n",
      "　ジャングルで、体育座りをしたフリアンニが、まだ、三歳にもなっていない頃の話である。\n",
      "　まだ魔法が使えなかった頃ーー\n",
      "　彼は、何が才能に満ち溢れているかと問われたら「運動」だと即答する少年だった。\n",
      "　ジャングルを駆け回る運動神経に優れた彼にとって、鼻で吸った息を口から吹くのは当然のこと。その「当然」が出来ないミーニャに、何度も笑われたことはあっても、不思議に思ったことはなかった。ミーニャが「その行為は胸を圧迫して苦しいからやめた方がいい」と提言されたときも、猿山フリアンニは「そんなことか」と軽く流した。\n",
      "　彼が３歳７ヶ月の頃の出来事であり、当然、胸を圧迫するというフレーズに一切の疑問を持たずに、ジャングルの木から\n"
     ]
    }
   ],
   "source": [
    "import \n",
    "input_ids = tokenizer.encode(\n",
    "    \"あなたは真面目な医者です。以下の質問に答えてください。「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\",\n",
    "    add_special_tokens=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    input_ids.to(device=model.device),\n",
    "    max_new_tokens=512,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "out = tokenizer.decode(tokens[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdb1e4-d496-423a-9648-319b26107d29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### stabilityai/japanese-stablelm-instruct-beta-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780c07bf-e580-45c1-9256-97eeae24d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba6244813444672bd13dd2d4b8c2215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "あなたは役立つアシスタントです。\n",
      "<<SYS>>\n",
      "\n",
      "以下の質問に答えてください。その時に与えられた情報を参照すること。\n",
      "    \n",
      "\n",
      "質問\n",
      "「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\n",
      "\n",
      "情報\n",
      "「中等度の気胸になった場合はカテーテルを肺と胸膜の間に入れ込んで、空気を出しつつ、回復するのを待つのが一般的です」\n",
      " [/INST]  はい\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"stabilityai/japanese-stablelm-instruct-beta-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# The next line may need to be modified depending on the environment\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "\n",
    "def build_prompt(user_query, inputs):\n",
    "    sys_msg = \"<s>[INST] <<SYS>>\\nあなたは役立つアシスタントです。\\n<<SYS>>\\n\\n\"\n",
    "    p = sys_msg + user_query + \"\\n\\n\" + inputs + \" [/INST] \"\n",
    "    return p\n",
    "\n",
    "user_inputs = {\n",
    "    \"user_query\": \"\"\"以下の質問に答えてください。その時に与えられた情報を参照すること。\n",
    "    \"\"\",\n",
    "    \"inputs\": \"\"\"質問\n",
    "「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\n",
    "\n",
    "情報\n",
    "「中等度の気胸になった場合はカテーテルを肺と胸膜の間に入れ込んで、空気を出しつつ、回復するのを待つのが一般的です」\n",
    "\"\"\"\n",
    "}\n",
    "prompt = build_prompt(**user_inputs)\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    prompt,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# this is for reproducibility.\n",
    "# feel free to change to get different result\n",
    "seed = 23  \n",
    "torch.manual_seed(seed)\n",
    "\n",
    "tokens = model.generate(\n",
    "    input_ids.to(device=model.device),\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.99,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "out = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76222f-9dde-4af1-a585-3ecc0765604f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### matsuo-lab/weblab-10b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfce1e8-e99a-4ce4-bf17-7f40aebe6368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58b08046ab432394bb07f44a4be3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb8c462202c45708b0fc6128a19955f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc60c49a73142d486b2a37d5c3c04e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34820a7bedc45d0961682e0a1285d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d26531459444e69f801183cb87119e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/47.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c982c4ba5e42c9872f27d2800e3c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a27bc3fbb14d09abcaf1ceafe3d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a246f87e87924838affc17010f05cf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/10.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5f023135224f4ea19bb0c607995ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/1.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf1f7fd36b84df19fa99d4ff1dc2e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f324bc228e1424ebc388ac1d3db4a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中等度の気胸になった場合、どのように治療するのが一般的ですか？\n",
      "気胸になった場合、胸腔ドレーンを入れて肺を圧迫する方法が一般的です。この方法は、中等度の気胸の場合、外科的に胸腔内にカテーテルを入れ、カテーテルから胸腔ドレーンを入れる方法よりも、確実に肺を圧迫することができます。ただし、肺を圧迫することによる副作用として、気胸が再発する場合があります。再発した場合は、再度カテーテルを入れて胸腔ドレーンを入れる必要があります。また、カテーテルによる圧迫は、患者さんにとっても負担が大きくなります。\n",
      "Q. 気胸が再発した場合、どのように治療するのが一般的ですか？\n",
      "気胸が再発した場合、再発した気胸を治療するために胸腔ドレーンを再度入れることもあります。しかし、再発した気胸は胸腔ドレーンを再度入れるよりも、外科的に胸腔内にカテーテルを入れ、カテーテルから胸腔ドレーンを入れる方が、確実に治療することができます。また、外科的に胸腔内にカテーテルを入れる方法の場合、カテーテルによる圧迫は、患者さんにとっても負担が大きくなります。\n",
      "Q. 気胸が再発した場合、カテーテルを使った治療はどれくらいの期間行うのが一般的ですか？\n",
      "気胸が再発した\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"matsuo-lab/weblab-10b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"matsuo-lab/weblab-10b\", torch_dtype=torch.float16)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "text = \"中等度の気胸になった場合、どのように治療するのが一般的ですか？\"\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca1e510-3c32-4fa3-8b6e-9195c7817c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation time: 20.70776391029358\n",
      "\n",
      "情報\n",
      "「中等度の気胸になった場合はカテーテルを肺と胸膜の間に入れ込んで、空気を出しつつ、回復するのを待つのが一般的です」\n",
      "\n",
      "以下の質問に答えてください。その時に上記の情報を参照すること。\n",
      "\n",
      "質問\n",
      "「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\n",
      "\n",
      "\n",
      "\n",
      "気胸の治療法には主に以下のものがある。\n",
      "\n",
      "カテーテルを挿入して空気を抜く。\n",
      "\n",
      "手術で胸膜を取り除く。\n",
      "\n",
      "胸腔鏡を使って胸膜を取り除く。\n",
      "\n",
      "中等度の気胸がある場合、カテーテルを挿入して空気を抜くのが一般的だ。\n",
      "\n",
      "カテーテルは肺と胸膜の間に挿入する。\n",
      "\n",
      "カテーテルを挿入するときは、胸部X線検査で肺の中に空気が溜まっているかどうかを確認する。\n",
      "\n",
      "カテーテルは肺の中に溜まった空気を抜く。\n",
      "\n",
      "カテーテルを抜くときは、胸部X線検査で肺の中に空気が溜まっているかどうかを確認する。\n",
      "\n",
      "カテーテルを抜くときは、胸部X線検査で肺の中に空気が溜まっているかどうかを確認する。\n",
      "\n",
      "カテーテルを抜くときは、胸部X線検査で肺の中に空気が溜まっているかどうかを確認する。\n",
      "\n",
      "気胸が重症化した場合、胸膜を取り除く手術が必要になることがある。\n",
      "\n",
      "胸膜を取り除く手術は、気胸がひどくなったときに行われる。\n",
      "\n",
      "胸膜を取り除く手術では、肺の中に溜まった空気を出しつつ、回復するのを待つ。\n",
      "\n",
      "胸膜を取り除く手術は、気�\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "情報\n",
    "「中等度の気胸になった場合はカテーテルを肺と胸膜の間に入れ込んで、空気を出しつつ、回復するのを待つのが一般的です」\n",
    "\n",
    "以下の質問に答えてください。その時に上記の情報を参照すること。\n",
    "\n",
    "質問\n",
    "「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tokenizer.pad_token_id = 0\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "print(f\"calculation time: {end-start}\")\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3e2b6-4a59-45b7-bc36-62d53c936a00",
   "metadata": {},
   "source": [
    "### cyberagent/calm2-7b-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc08161-1260-44c0-848c-f6b6f389aa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2df5c304427411d8204e1930264d0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f653879c7244fcd88be2339f5aebcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af6721443f445eb8047bd3a158194d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be697e7c974e45e98327ca700e304cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad311d60cbf46a8a788e4ed05a44a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0f5033162f408e88556186978d6f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26290cd343a54a158b61f13768a13fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fe588531e24be294ab0e3071121219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d111600f767742a78bd3d102e10ad808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2328fabbb3645f6ac687d8f1270aab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能、AIは私達の暮らしを大きく変える可能性があります。以下のような分野で大きな変化をもたらすと言われています。\n",
      "\n",
      "1. 医療\n",
      "AIによって医療現場での診断や治療方法の改善が可能になります。例えば、AIを使った画像解析技術によって、患者の微細な変化も見逃さない高度な診断が可能になるかもしれません。また、AIを活用した創薬研究も進んでおり、病気の原因となる蛋白質の解明や治療薬の開発への応用が期待されています。\n",
      "2. 金融\n",
      "AIは、金融業界に大きな影響を与えます。融資の審査や不正行為の検出など、金融サービスにおける業務の効率化やリスク管理の向上が期待されています。また、AIを利用した資産運用も増えており、より多くの投資家から資金を集めやすい環境が生まれるかもしれません。\n",
      "3. 交通・物流\n",
      "自動運転技術の開発によって、交通システムの効率化や安全性の向上が期待されています。また、物流においては、自動仕分けシステムや貨物追跡システムなどの導入が進んでおり、物流業界における省人化や生産性の向上が期待されています。\n",
      "\n",
      "AIは今後も開発が進むことが予想され、私たちの暮らしはより便利で豊かになるかもしれません。一方で、AIがもたらす影響については、倫理的な問題や社会への影響など、多角的な検討が必要です。\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "assert transformers.__version__ >= \"4.34.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/calm2-7b-chat\", device_map=\"auto\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/calm2-7b-chat\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "prompt = \"\"\"USER: AIによって私達の暮らしはどのように変わりますか？\n",
    "ASSISTANT: \"\"\"\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids=token_ids.to(model.device),\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    streamer=streamer,\n",
    ")\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4029b65-1c28-4150-921c-ef7eec13169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "軽度な気胸の場合には、多くの場合自然治癒が期待できるため、自宅療養で様子を見ることも可能です。しかし、中等度の気胸の場合、自然治癒が難しいケースがあります。\n",
      "\n",
      "中等度の気胸の場合には、以下のような治療が行われることが多いです。\n",
      "\n",
      "1.胸腔ドレナージ：胸腔ドレナージは、チューブを使って胸腔内から空気を抜く治療方法です。胸腔ドレナージにより、気胸の原因となっている肺から空気が漏れ出るのを防ぎ、肺を修復することができます。\n",
      "\n",
      "2.手術：手術によって、炎症を起こしている肺を切除したり、穴をふさいだりする治療方法もあります。\n",
      "\n",
      "いずれにしても、気胸の原因は一人ひとり異なりますので、治療方針は医師と相談して決めましょう。\n",
      "USER: 中等度の気胸になった場合、どのように治療するのが一般的ですか？\n",
      "ASSISTANT: 軽度な気胸の場合には、多くの場合自然治癒が期待できるため、自宅療養で様子を見ることも可能です。しかし、中等度の気胸の場合、自然治癒が難しいケースがあります。\n",
      "\n",
      "中等度の気胸の場合には、以下のような治療が行われることが多いです。\n",
      "\n",
      "1.胸腔ドレナージ：胸腔ドレナージは、チューブを使って胸腔内から空気を抜く治療方法です。胸腔ドレナージにより、気胸の原因となっている肺から空気が漏れ出るのを防ぎ、肺を修復することができます。\n",
      "\n",
      "2.手術：手術によって、炎症を起こしている肺を切除したり、穴をふさいだりする治療方法もあります。\n",
      "\n",
      "いずれにしても、気胸の原因は一人ひとり異なりますので、治療方針は医師と相談して決めましょう。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"USER: 中等度の気胸になった場合、どのように治療するのが一般的ですか？\n",
    "ASSISTANT: \"\"\"\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids=token_ids.to(model.device),\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    streamer=streamer,\n",
    ")\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea1b4a1-dba1-45f2-8340-f0908fb57ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一般的な治療方法は、カテーテルを肺と胸膜の間に入れ込んで、空気を出すことです。これによって、肺が自然に回復する力を助けることができます。また、胸膜腔を洗浄するために、薬剤を注入することもあります。\n",
      "calculation time: 1.6182489395141602\n",
      "USER: 情報\n",
      "「中等度の気胸になった場合はカテーテルを肺と胸膜の間に入れ込んで、空気を出しつつ、回復するのを待つのが一般的です」\n",
      "\n",
      "以下の質問に答えてください。その時に上記の情報を参照すること。\n",
      "\n",
      "質問\n",
      "「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\n",
      "ASSISTANT: 一般的な治療方法は、カテーテルを肺と胸膜の間に入れ込んで、空気を出すことです。これによって、肺が自然に回復する力を助けることができます。また、胸膜腔を洗浄するために、薬剤を注入することもあります。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "prompt = \"\"\"USER: 情報\n",
    "「中等度の気胸になった場合はカテーテルを肺と胸膜の間に入れ込んで、空気を出しつつ、回復するのを待つのが一般的です」\n",
    "\n",
    "以下の質問に答えてください。その時に上記の情報を参照すること。\n",
    "\n",
    "質問\n",
    "「中等度の気胸になった場合、どのように治療するのが一般的ですか？」\n",
    "ASSISTANT: \"\"\"\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "output_ids = model.generate(\n",
    "    input_ids=token_ids.to(model.device),\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    streamer=streamer,\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "print(f\"calculation time: {end-start}\")\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76bdfc8-0cf4-4e63-8e45-e0443cca42c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91968761574d4124be0ab6d7db77fee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724f7e026254ca69b8c978f1abcc587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ba20acea5348fdac6988bf9aaa26aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3c0d69e1c345ef92514b87d04eb6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bcc70aa7574ea883a3a896c1017a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e374579a0d4544b283ff2f9cb900e27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f10abfa1bd34128b642747d77bb1800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212f075d310f4f099256fa452195a0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e32c97967644867a963f2cc967fbc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26bcdc74de14455a6d28f446344dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cmH2Oに設定したい場合に、吸引部に何mLの滅菌水を注ぎ入れるべきかは、仕様書から読み取ることはできません。具体的な数字は、具体的な使用方法や機器の設定値に依存するためです。ただし、5cmH2Oは、非常に低い吸引圧を意味するため、通常、非常に高い流量が必要な場合に使用されます。このため、具体的な使用方法は不明ですが、使用機器の設定値や、流量や吸引圧の要求により、適切な吸引圧と流量を決定する必要があります。\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "assert transformers.__version__ >= \"4.34.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/calm2-7b-chat\", device_map=\"auto\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/calm2-7b-chat\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "prompt = \"\"\"USER: 以下の仕様書を参考にして、その次の問いに答えてください。\n",
    "\n",
    "```仕様書\n",
    "チェスト・ドレーン・バッグ（住友ベークライト）の使い方：\n",
    "　５つの部屋があり、正面から見て左から一つ目、二つ目、三つ目の部屋が体内から排液を貯めて置く場所、右から一つ目と二つ目が吸引圧制御の部屋\n",
    "\n",
    "　吸引部の設定\n",
    "　　一番右の部屋には滅菌蒸留水（黄色）を入れる。目安; 20cmH2O:182mL、15cmH2O:130mL、10cmH2O:78mL、5cmH2O:25mL\n",
    "　　吸引圧調節中は空気導入口を防ぐことになるため、調節時は吸引源の陰圧を停止すること。停止しない場合、吸引装置の圧力が胸腔内に直接かかり大変危険です\n",
    "\n",
    "　水封の設定\n",
    "　　黄色ラベルの貼られた吸引装置接続チューブより水封部に滅菌蒸留水（青色）を30mL注水します。\n",
    "　　水位の調節\n",
    "　　　水封止水位調節ポートに注射針つきシリンジを差し込むと水位の調節が可能です。\n",
    "\n",
    "　気密性の確認\n",
    "　　１、水封部・吸引部への注水後、今日食うドレーン接続チューブをクランプします\n",
    "　　２、黄色ラベルの貼られた吸引装置接続チューブのコネクタと吸引装置を接続します\n",
    "　　３、吸引のスイッチをオンにし、徐々に吸引量を上げると水封部（青色）、吸引部（黄色）の順番に気泡が発生します\n",
    "　　４、吸引部の気泡が確認できたら、吸引装置接続コネクタを吸引装置から外します\n",
    "　　５、吸引装置を外した際、水封部の青色の水が細管を上昇し、20~30秒間静止することを確認することで機密性の確認が行えます。\n",
    "\n",
    "　ドレナージチューブ留置部の管理\n",
    "　　ナイロン糸などで皮膚を固定し、テガダーム等の透明なドレッシング材で刺入部を多います\n",
    "```\n",
    "\n",
    "5cmH2Oに設定したいときに、吸引部に滅菌水を何mL入れればいいですか？\n",
    "\n",
    "ASSISTANT: \"\"\"\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids=token_ids.to(model.device),\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4708a87d-2b7b-4389-9744-21ffa1bd1f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吸引部には25mLの滅菌水が必要です。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"USER: 以下の仕様書を参考にして、その次の問いに答えてください。\n",
    "\n",
    "```仕様書\n",
    "　吸引部の設定\n",
    "　　一番右の部屋には滅菌蒸留水（黄色）を入れる。目安; 20cmH2O:182mL、15cmH2O:130mL、10cmH2O:78mL、5cmH2O:25mL\n",
    "　　吸引圧調節中は空気導入口を防ぐことになるため、調節時は吸引源の陰圧を停止すること。停止しない場合、吸引装置の圧力が胸腔内に直接かかり大変危険です\n",
    "```\n",
    "\n",
    "5cmH2Oに設定したいときに、吸引部に滅菌水を何mL入れればいいですか？\n",
    "\n",
    "ASSISTANT: \"\"\"\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids=token_ids.to(model.device),\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6899d-12ab-48e8-bcd2-b21f8e890d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e302ff97-bd81-418c-9c40-eccf672ed98d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mistral Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e54d27-bb85-4913-945b-1349b7e8cd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e56f0fe0be434da2148e5d6de036ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b66dd367734a95b44c517894788a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129fba5673d94a9c921b9559087611f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11930799b744c33a77d332f5f12b0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28edbf2722d46d3ab6a2933405ccd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdd4245c7844f6483e7d956cdeb2999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bdadb14a1b4445bc4cdd64dadb9bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3790b4a1d34a818778db5909fb4b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c159d003954e2f9a0dae95326105b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0234541cf78347d08190c4d31d86da8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f631125bb7a8442c9735f1940fef244d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be5ca77e79f40d199a0a84bcf086593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] I'm an artificial intelligence, I don't consume food or cook. However, I can certainly help you find a simple and delicious homemade mayonnaise recipe! Here it is:\n",
      "\n",
      "Ingredients:\n",
      "- 1 egg yolk\n",
      "- 1 tablespoon Dijon mustard\n",
      "- 1 cup vegetable oil (preferably safflower or canola)\n",
      "- 1-2 tablespoons white wine vinegar or fresh lemon juice\n",
      "- Salt, to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a medium-sized bowl, whisk together the egg yolk and Dijon mustard.\n",
      "2. Slowly add the oil to the yolk mixture, whisking continuously to emulsify the oil and yolk. You'll want to add the oil in a thin stream, using a drip or fork to control the flow.\n",
      "3. Once the oil has been fully incorporated, add the vinegar or lemon juice in a steady stream, continuing to whisk constantly.\n",
      "4. Taste the mayonnaise and add salt as desired.\n",
      "5. If you prefer a thicker mayonnaise, you can add more oil a little at a time.\n",
      "\n",
      "Remember, it's important to use a clean, dry bowl and utensils to make mayonnaise, as any water or moisture can cause the mixture to curdle. Enjoy your homemade mayonnaise!</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be92ff61-98fb-4b96-a87c-c43dba4392b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation time: 15.586024045944214\n",
      "<s> \n",
      "[INST] What should we do when we got pneumothorax? [/INST]\n",
      "Pneumothorax is a medical condition where there is a buildup of air in the pleural cavity, causing the lung to collapse. It can be life-threatening if not treated promptly. If you suspect that you have pneumothorax, you should seek emergency medical help immediately. Here are some steps that can be taken before reaching the hospital:\n",
      "\n",
      "1. Sit up straight and take deep breaths: This can help expand the lungs and improve oxygenation.\n",
      "2. Seek emergency medical help: Call your doctor or emergency services and explain your symptoms.\n",
      "3. Position yourself on your side: This position can help the affected lung expand and improve breathing.\n",
      "4. Avoid strenuous activity: Avoid any physical activity that may increase your heart rate or breathing rate, as this can worsen the condition.\n",
      "5. Monitor your symptoms: Keep track of your symptoms, including the severity of chest pain, shortness of breath, and heart rate.\n",
      "\n",
      "Once you reach the hospital, doctors may use various treatments to relieve the pressure in the pleural cavity, such as a chest tube or needle thoracentesis. In some cases, surgery may be necessary to repair the underlying cause of the pneumothorax. It's important to follow your doctor's instructions carefully and attend all follow-up appointments to ensure proper healing and prevent future episodes.</s>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text = \"\"\"\n",
    "[INST] What should we do when we got pneumothorax? [/INST]\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors = \"pt\", add_special_tokens = True,).to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=1000)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"calculation time: {end-start}\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b316e78-6153-488c-84d0-70500899f2b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation time: 9.202413082122803\n",
      "<s> \n",
      "[INST] What should we do when we got pneumothorax? You should answer the question following the information bellow.\n",
      "\n",
      "information:\n",
      "```\n",
      "If you have a moderate pneumothorax, it is common to insert a catheter between the lung and pleura, let air out, and wait for the patient to recover.\n",
      "```\n",
      "[/INST]\n",
      "If you suspect that you have a pneumothorax and it is not an emergency situation with severe symptoms such as trouble breathing, chest pain that worsens with deep breaths or coughing, or bluish skin, then you should seek medical attention as soon as possible. Once you have been diagnosed with a moderate pneumothorax, the typical treatment is to insert a catheter between the lung and the pleura to let the air out. This procedure is called a chest tube thoracostomy. The catheter is left in place to allow the lung to re-expand and the air to continue escaping. You will be monitored closely in the hospital while you recover. It is important to follow your healthcare provider's instructions carefully and attend all follow-up appointments to ensure proper healing.</s>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text = \"\"\"\n",
    "[INST] What should we do when we got pneumothorax? You should answer the question following the information bellow.\n",
    "\n",
    "information:\n",
    "```\n",
    "If you have a moderate pneumothorax, it is common to insert a catheter between the lung and pleura, let air out, and wait for the patient to recover.\n",
    "```\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors = \"pt\", add_special_tokens = True,).to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=1000)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"calculation time: {end-start}\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409721b-f07d-4da3-88ad-acfdf47702b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8825e-5d99-4417-99e8-87f777b7855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all datasets with .tex, .txt are in \"./database\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5726a04-03e1-49cd-bf81-ada88f61f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"cyberagent/calm2-7b-chat\"\n",
    "max_tokens = 100\n",
    "min_tokens = 30  # this should be longer enough than keys of rules bellow since this should play an role to exclude the case of the process text starts with the key.(if the process text starts with the key, that can cause the \"shit happend\" in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a6b886-f4bb-4706-aac0-eaebd7b3147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c75a57-a351-4e18-9ab6-3035092dfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./doc_KIKYO.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# about where the key starts to split the text\n",
    "\n",
    "# index : words to be where text should be splited\n",
    "# first element(0 to 1): process_text_size*element is the start point of the key splitting. the samller the element is, the more likely it is for the key to split the text.\n",
    "# second element(0 or 1): the first element should become   if 0: <text1><key> | <text2>,  if 1: <text1> | <key><text2>\n",
    "\"\"\" smaple\n",
    "rules = [\n",
    "    {\n",
    "        \"\\\\\\\\section*\" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\\\\\\\subsection*\" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\\\\\\\begin{center}\" : 1,\n",
    "        \"\\\\\\\\end{gather*}\" : 0,\n",
    "        \"\\\\\\\\end{align*}\" : 0,   \n",
    "        \"\\\\\\\\end{equation*}\" : 0,\n",
    "        \"\\\\\\\\end{enumerate}\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\\n\" : 0,\n",
    "        \"<0x0A><0x0A>\" : 0,\n",
    "        \"\\x0A\\x0A\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\" : 0,\n",
    "        \"<0x0A>\" : 0,\n",
    "        \"\\x0A\" : 0,\n",
    "    },\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "rules = [\n",
    "    {\n",
    "        \"/\" : 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "# 注：latexで\\\\となっているところでsplitしたい場合、ruleのkeyには\\\\\\\\と記述しないといけない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a52e6ef8-cf04-434a-b8d2-a1c6cbc59573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process time:  0.019797563552856445\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def split_into_chunks(tokenizer, text, max_tokens, min_tokens, rules):\n",
    "    chunk_list = []\n",
    "    instruction_list = []\n",
    "    \n",
    "    start_id = 0\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", add_special_tokens = False)\n",
    "    num_tokens = len(tokenized_text[\"input_ids\"][0])\n",
    "    text_size = len(text)\n",
    "    \n",
    "    for i in range(int(num_tokens/min_tokens)+1):\n",
    "        if(start_id + max_tokens >= num_tokens):\n",
    "            process_tokenized_text = tokenized_text[\"input_ids\"][0][start_id:]\n",
    "            processed_text = tokenizer.decode(process_tokenized_text, skip_special_tokens=True)\n",
    "            chunk_list.append(processed_text)\n",
    "            instruction_list.append(processed_text)\n",
    "            break\n",
    "            \n",
    "        process_tokenized_text = tokenized_text[\"input_ids\"][0][start_id : start_id + max_tokens]\n",
    "        process_text = tokenizer.decode(process_tokenized_text, skip_special_tokens=True) # this should be decoded since subword token is difficult to handle\n",
    "        process_text_size = len(process_text)\n",
    "\n",
    "        #determine where should be split\n",
    "        min_split_text = process_text_size\n",
    "        is_text_split = False\n",
    "        for rule_group in rules:\n",
    "            for rule_key in rule_group.keys():\n",
    "                split_process_text = process_text.split(rule_key)\n",
    "                if len(split_process_text) > 1:\n",
    "                    size_last_split_process_text = len(split_process_text[-1])\n",
    "                    if (size_last_split_process_text < min_split_text) and (size_last_split_process_text < (1 - min_tokens/max_tokens)*process_text_size) and (size_last_split_process_text!=process_text_size):\n",
    "                        is_text_split = True\n",
    "                        min_split_text = size_last_split_process_text + len(rule_key) * rule_group[rule_key]\n",
    "\n",
    "            if is_text_split:\n",
    "                break\n",
    "                        \n",
    "        if is_text_split:\n",
    "            processed_text = process_text[:-min_split_text]\n",
    "        else:\n",
    "            processed_text = process_text\n",
    "\n",
    "        \n",
    "        processed_tokenized_text = tokenizer(processed_text, return_tensors=\"pt\", add_special_tokens = False)\n",
    "        len_processed_text = len(processed_tokenized_text[\"input_ids\"][0])  #this could be more than max_tokens without min sentence, which caused fatal error\n",
    "\n",
    "        if len(processed_text)==0:\n",
    "            break\n",
    "            \n",
    "        chunk_list.append(processed_text)  \n",
    "        instruction_list.append(processed_text)\n",
    "        \n",
    "        start_id += len_processed_text # taking from process_tokenized_text to prevent the id from getting wrong\n",
    "\n",
    "    return chunk_list, instruction_list\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "chunks, insts = split_into_chunks(tokenizer, text, max_tokens, min_tokens, rules)\n",
    "end = time.time()\n",
    "\n",
    "print(\"process time: \", end-start)\n",
    "\n",
    "input_file_path = \"chunks.json\"\n",
    "with open(input_file_path, 'w') as json_file:\n",
    "    json.dump(chunks, json_file)\n",
    "\n",
    "# chunks: [<str> chunk of the text, ...]\n",
    "# insts: [<str> instructions corresponds to chunk, ...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d76779-a4fe-44fd-a2b4-3d30a0b640fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3882493-2c00-418b-a565-503f0b9741d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647e31ed-1336-4e63-b2a2-24434b00dfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/\\n気胸になった時に感じること：胸の奥の締め付けられるような痛み、軽く運動をした時に激しく...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nチェスト・ドレーン・バッグ（住友ベークライト）の使い方：\\n　５つの部屋があり、正面から...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n　吸引部の設定\\n　　一番右の部屋には滅菌蒸留水（黄色）を入れる。目安; 20cmH2O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>胸腔内に直接かかり大変危険です\\n/\\n　水封の設定\\n　　黄色ラベルの貼られた吸引装置接続...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n　気密性の確認\\n　　１、水封部・吸引部への注水後、今日食うドレーン接続チューブをクラン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>クタを吸引装置から外します\\n　　５、吸引装置を外した際、水封部の青色の水が細管を上昇し、2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nチェスト・ドレーン・バッグ（住友ベークライト）の使い方：\\n　本体使用時の管理\\n/\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n　・バッグ本体は傾けない倒さない\\n/\\n　・空気導入口は塞がない\\n　　　空気導入口を...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n　・吸引装置の圧力は吸引部内に連続した少量の気泡が出るように調節すること（吸引装置の陰圧...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n　・患者が移動する際は排液チューブをクランプしない\\n　　突然咳き込んだりした際に胸腔か...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  /\\n気胸になった時に感じること：胸の奥の締め付けられるような痛み、軽く運動をした時に激しく...\n",
       "1  \\nチェスト・ドレーン・バッグ（住友ベークライト）の使い方：\\n　５つの部屋があり、正面から...\n",
       "2  \\n　吸引部の設定\\n　　一番右の部屋には滅菌蒸留水（黄色）を入れる。目安; 20cmH2O...\n",
       "3  胸腔内に直接かかり大変危険です\\n/\\n　水封の設定\\n　　黄色ラベルの貼られた吸引装置接続...\n",
       "4  \\n　気密性の確認\\n　　１、水封部・吸引部への注水後、今日食うドレーン接続チューブをクラン...\n",
       "5  クタを吸引装置から外します\\n　　５、吸引装置を外した際、水封部の青色の水が細管を上昇し、2...\n",
       "6  \\nチェスト・ドレーン・バッグ（住友ベークライト）の使い方：\\n　本体使用時の管理\\n/\\n...\n",
       "7  \\n　・バッグ本体は傾けない倒さない\\n/\\n　・空気導入口は塞がない\\n　　　空気導入口を...\n",
       "8  \\n　・吸引装置の圧力は吸引部内に連続した少量の気泡が出るように調節すること（吸引装置の陰圧...\n",
       "9  \\n　・患者が移動する際は排液チューブをクランプしない\\n　　突然咳き込んだりした際に胸腔か..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make pandas dataframe\n",
    "import json\n",
    "\n",
    "json_file_path = \"./chunks.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    sentences = json.load(json_file)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'sentences': sentences})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e708ac-f876-46b1-8bfc-d54040431d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape :  torch.Size([10, 1024])\n",
      "200000.0 % finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "def get_cat_list(series):\n",
    "    cat_list = []\n",
    "    num_rows = 0\n",
    "    for text_list in series:\n",
    "        num_rows += 1\n",
    "        cat_list += text_list\n",
    "    return cat_list, num_rows\n",
    "\"\"\"\n",
    "\n",
    "def save_outputs(text_series, batch_size, file_path):\n",
    "    num_batch = 0\n",
    "\n",
    "    for i in range(0, len(text_series), batch_size):\n",
    "        batch_text_series = text_series[i:i+batch_size]\n",
    "        num_rows = len(batch_text_series)\n",
    "        #inputs = tokenizer(batch_text_series, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "        #inputs.to(device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = torch.tensor(model.encode(batch_text_series))\n",
    "            #outputs = outputs.reshape(num_rows, max_length, -1)\n",
    "            print(\"outputs.shape : \", outputs.shape)\n",
    "            torch.save(outputs.detach().cpu(), file_path+str(num_batch)+\".pt\")\n",
    "            num_batch += 1\n",
    "\n",
    "            del outputs\n",
    "        \n",
    "        print(100*(i+batch_size)/len(text_series), \"% finished\")\n",
    "\n",
    "#batch_size_questions = 100  # Adjust based on your GPU memory\n",
    "batch_size = 20000\n",
    "\n",
    "#max_length_questions = 10  # Set all token length same\n",
    "\n",
    "# Adjust considering cpu memory. torch.save and torch.load takes much time,\n",
    "# so output should be accumulated in cpu memory as much as possible.\n",
    "# ex. total size of data is around 400GB and cpu memory is around 58GB,\n",
    "# therefore, around 10% of data can be accumulated in cpu memory. \n",
    "#save_rate = 1\n",
    "\n",
    "# here each list in df[\"chunks\"] must have same length\n",
    "save_outputs(df[\"sentences\"], batch_size, \"embs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d55b71-d6eb-4e91-b7ef-a3d54c8a1d6f",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1144b76f-f07a-4cf3-af93-8dd34a83451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d926ef6b25641d3acd27cc99188fbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7d9ddef4bd49818bd4e42044c1b32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07786bc22afa4b8aab53943d3f0a0be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59a397325924d22acf6c7c1ddb65146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0459067d258b4110950d09659626696a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac022f6e5de946428aebc8c36e12b579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2229901f6cfa4c52a94097d6ccc67015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43d67ebd3e04f1bb52ed99fd8b96d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b3e7092efd4ac788646c49f3551889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a73ac60924548d2b5a9993919ef103a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     47\u001b[0m         prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m inf\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(questions):\n\u001b[1;32m     53\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m get_prompt(questions[i], infs[i])\n\u001b[1;32m     54\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "emb_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\").to(device)\n",
    "\n",
    "questions = [\n",
    "    \"チェストドレインの使用上の注意事項を教えてください。\",\n",
    "    \"5cmH2Oに設定したいときに、吸引部に滅菌水を何mL入れればいいですか？\",\n",
    "]\n",
    "\n",
    "q_embs = torch.tensor(emb_model.encode(questions)).to(device)\n",
    "q_embs = q_embs.reshape(q_embs.shape[0], q_embs.shape[1])\n",
    "inf_embs = torch.load(\"embs0.pt\").to(device)\n",
    "\n",
    "relevance = torch.matmul(q_embs, inf_embs.T)\n",
    "\n",
    "values, inf_ids = torch.topk(relevance, k=3, dim=1)\n",
    "\n",
    "infs = [[] for _ in range(relevance.shape[0])]\n",
    "rels = [[] for _ in range(relevance.shape[0])]\n",
    "\n",
    "import json\n",
    "json_file_path = \"./chunks.json\"\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    sentences_list = json.load(json_file)\n",
    "\n",
    "for i in range(len(inf_ids)):\n",
    "    for id in inf_ids[i]:\n",
    "        infs[i].append(sentences_list[id])\n",
    "    for rel in values[i]:\n",
    "        rels[i].append(rel)\n",
    "\n",
    "# Model load\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "assert transformers.__version__ >= \"4.34.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/calm2-7b-chat\", device_map=\"auto\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/calm2-7b-chat\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def get_prompt(q, inf_list):\n",
    "    prompt = q + \"\\n必要であれば以下の情報を参照してください\"\n",
    "    for inf in inf_list:\n",
    "        prompt += \"\\n\" + inf\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d7dc190-bb6b-4095-99aa-10cf2d7233b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infs(question):\n",
    "    q_embs = torch.tensor(emb_model.encode(question)).to(device)\n",
    "    #q_embs = q_embs.reshape(q_embs.shape[0], q_embs.shape[1])\n",
    "    inf_embs = torch.load(\"embs0.pt\").to(device)\n",
    "\n",
    "    \n",
    "    relevance = torch.matmul(q_embs, inf_embs.T)\n",
    "    \n",
    "    values, inf_ids = torch.topk(relevance, k=3, dim=0)\n",
    "    \n",
    "    infs = []\n",
    "    \n",
    "    import json\n",
    "    json_file_path = \"./chunks.json\"\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        sentences_list = json.load(json_file)\n",
    "    \n",
    "    for id in inf_ids:\n",
    "        infs.append(sentences_list[id])\n",
    "\n",
    "    return infs\n",
    "\n",
    "def get_prompt(q, inf_list):\n",
    "    prompt = q + \"\\n必要であれば以下の情報を参照してください\\n```\"\n",
    "    for inf in inf_list:\n",
    "        prompt += inf + \"\\n\"\n",
    "    prompt += \"```\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890b8194-33f3-4e5c-966e-d22bf6fa6db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\n",
      "チェストドレインの使用上の注意事項を教えてください。\n",
      "\n",
      "必要であれば以下の情報を参照してください\n",
      "```\n",
      "　・患者が移動する際は排液チューブをクランプしない\n",
      "　　突然咳き込んだりした際に胸腔からでたエアの逃げ道がなくなります\n",
      "　　クランプすると胸腔内に逆流し、肺虚脱を再発します\n",
      "\n",
      "　気密性の確認\n",
      "　　１、水封部・吸引部への注水後、今日食うドレーン接続チューブをクランプします\n",
      "　　２、黄色ラベルの貼られた吸引装置接続チューブのコネクタと吸引装置を接続します\n",
      "　　３、吸引のスイッチをオンにし、徐々に吸引量を上げると水封部（青色）、吸引部（黄色）の順番に気泡が発生します\n",
      "　　４、吸引部の気泡が確認できたら、吸引装置接続コネ\n",
      "\n",
      "チェスト・ドレーン・バッグ（住友ベークライト）の使い方：\n",
      "　本体使用時の管理\n",
      "/\n",
      "　・水封部に規定通りの水量が注入されているか定期的に確認する\n",
      "　　　水封部の水がなくなるとウォーターシールの効果がなくなります\n",
      "　　　水封部に注水されていないと、外科医と胸腔内が直接繋がり、陰圧の空気が逆流して肺の虚脱が起こったり、換気が妨げられるので、注意が必要です\n",
      "/\n",
      "```\n",
      "--------\n",
      "\n",
      "ASSISTANT: チェストドレインを使用する際の注意事項として、以下のことが挙げられます。\n",
      "\n",
      "1. 患者が移動する際は、排液チューブをクランプしないこと。突然咳き込んだりした際に胸腔からでたエアの逃げ道がなくなります。クランプすると胸腔内に逆流して、肺虚脱を再発します。\n",
      "2. 気密性の確認が必要です。水封部・吸引部への注水後、吸引器接続チューブをクランプします。黄色ラベルの貼られた吸引装置接続チューブのコネクタと吸引装置を接続し、吸引のスイッチをオンにし、徐々に吸引量を上げると、水封部（青色）、吸引部（黄色）の順番に気泡が発生します。吸引部の気泡が確認できたら、吸引装置接続コネクタと吸引装置を接続します。吸引のスイッチをオンにし、徐々に吸引量を上げると、水封部（青色）、吸引部（黄色）の順番に気泡が発生します。吸引部の気泡が確認できたら、吸引装置接続コネクタと吸引装置を接続します。吸引のスイッチをオンにし、徐々に吸引量を上げると、水封部（青色）、吸引部（黄色）の順番に気泡が発生します。吸引部の気泡が確認できたら、吸引装置接続コネクタと吸引装置を接続します。吸引のスイッチをオンにし、徐々に吸引量を上げると、水封部（青色）、吸引部（黄色）の順番に気泡が発生します。吸引部の気\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "チェストドレインの使用上の注意事項を教えてください。\n",
    "\"\"\"\n",
    "\n",
    "infs = get_infs(question)\n",
    "prompt = get_prompt(question, infs)\n",
    "\n",
    "print(\"--------\")\n",
    "print(prompt)\n",
    "print(\"--------\")\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids=token_ids.to(model.device),\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500fabe-9cdc-45c2-b2ed-deaa3990bdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d360e-ba76-41d2-b36c-95a9b90cb8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32855074-6d21-467f-a7f0-a933d363dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "emb_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "881dc466-9a74-411e-a3f6-cce1c8c11257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1024]), torch.Size([10, 1024]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "questions = [\n",
    "    \"チェストドレインの使用上の注意事項を教えてください。\",\n",
    "    \"5cmH2Oに設定したいときに、吸引部に滅菌水を何mL入れればいいですか？\",\n",
    "]\n",
    "\n",
    "q_embs = torch.tensor(emb_model.encode(questions)).to(device)\n",
    "q_embs = q_embs.reshape(q_embs.shape[0], q_embs.shape[1])\n",
    "inf_embs = torch.load(\"embs0.pt\").to(device)\n",
    "\n",
    "q_embs.shape, inf_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e18c1411-5d1f-48ae-8a6d-6f0aff77468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = torch.matmul(q_embs, inf_embs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16542581-14fd-450f-92af-e611bc8118cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------informations for question0 -----------\n",
      "question : チェストドレインの使用上の注意事項を教えてください。\n",
      "--relevance 265.30877685546875\n",
      "--information 0--\n",
      "\n",
      "　・患者が移動する際は排液チューブをクランプしない\n",
      "　　突然咳き込んだりした際に胸腔からでたエアの逃げ道がなくなります\n",
      "　　クランプすると胸腔内に逆流し、肺虚脱を再発します\n",
      "---------\n",
      "--relevance 265.2657470703125\n",
      "--information 1--\n",
      "\n",
      "　気密性の確認\n",
      "　　１、水封部・吸引部への注水後、今日食うドレーン接続チューブをクランプします\n",
      "　　２、黄色ラベルの貼られた吸引装置接続チューブのコネクタと吸引装置を接続します\n",
      "　　３、吸引のスイッチをオンにし、徐々に吸引量を上げると水封部（青色）、吸引部（黄色）の順番に気泡が発生します\n",
      "　　４、吸引部の気泡が確認できたら、吸引装置接続コネ\n",
      "---------\n",
      "--relevance 260.50201416015625\n",
      "--information 2--\n",
      "\n",
      "チェスト・ドレーン・バッグ（住友ベークライト）の使い方：\n",
      "　本体使用時の管理\n",
      "/\n",
      "　・水封部に規定通りの水量が注入されているか定期的に確認する\n",
      "　　　水封部の水がなくなるとウォーターシールの効果がなくなります\n",
      "　　　水封部に注水されていないと、外科医と胸腔内が直接繋がり、陰圧の空気が逆流して肺の虚脱が起こったり、換気が妨げられるので、注意が必要です\n",
      "/\n",
      "---------\n",
      "\n",
      "-----------informations for question1 -----------\n",
      "question : 5cmH2Oに設定したいときに、吸引部に滅菌水を何mL入れればいいですか？\n",
      "--relevance 276.834228515625\n",
      "--information 0--\n",
      "\n",
      "　気密性の確認\n",
      "　　１、水封部・吸引部への注水後、今日食うドレーン接続チューブをクランプします\n",
      "　　２、黄色ラベルの貼られた吸引装置接続チューブのコネクタと吸引装置を接続します\n",
      "　　３、吸引のスイッチをオンにし、徐々に吸引量を上げると水封部（青色）、吸引部（黄色）の順番に気泡が発生します\n",
      "　　４、吸引部の気泡が確認できたら、吸引装置接続コネ\n",
      "---------\n",
      "--relevance 274.96954345703125\n",
      "--information 1--\n",
      "胸腔内に直接かかり大変危険です\n",
      "/\n",
      "　水封の設定\n",
      "　　黄色ラベルの貼られた吸引装置接続チューブより水封部に滅菌蒸留水（青色）を30mL注水します。\n",
      "　　水位の調節\n",
      "　　　水封止水位調節ポートに注射針つきシリンジを差し込むと水位の調節が可能です。\n",
      "/\n",
      "---------\n",
      "--relevance 272.41668701171875\n",
      "--information 2--\n",
      "\n",
      "　吸引部の設定\n",
      "　　一番右の部屋には滅菌蒸留水（黄色）を入れる。目安; 20cmH2O:182mL、15cmH2O:130mL、10cmH2O:78mL、5cmH2O:25mL\n",
      "　　吸引圧調節中は空気導入口を防ぐことになるため、調節時は吸引源の陰圧を停止すること。停止しない場合、吸引装置の圧力が\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "values, inf_ids = torch.topk(relevance, k=3, dim=1)\n",
    "\n",
    "infs = [[] for _ in range(relevance.shape[0])]\n",
    "rels = [[] for _ in range(relevance.shape[0])]\n",
    "\n",
    "import json\n",
    "json_file_path = \"./chunks.json\"\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    sentences_list = json.load(json_file)\n",
    "\n",
    "for i in range(len(inf_ids)):\n",
    "    for id in inf_ids[i]:\n",
    "        infs[i].append(sentences_list[id])\n",
    "    for rel in values[i]:\n",
    "        rels[i].append(rel)\n",
    "\n",
    "\n",
    "for i in range(relevance.shape[0]):\n",
    "    print()\n",
    "    print(f\"-----------informations for question{i} -----------\")\n",
    "    print(f\"question : {questions[i]}\")\n",
    "    for j in range(len(infs[i])):\n",
    "        print(f\"--relevance {rels[i][j]}\")\n",
    "        print(f\"--information {j}--\")\n",
    "        print(infs[i][j])\n",
    "        print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229e919-06e4-4f01-ad2e-5ed2f36eed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's quite interesting fact that the instruction should be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a077531-aa34-440f-a239-de6b4d23df31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
